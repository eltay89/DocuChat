# ═══════════════════════════════════════════════════════════════════════════════
# DocuChat Configuration File
# ═══════════════════════════════════════════════════════════════════════════════
# 
# This file contains all configuration options for the DocuChat application.
# You can override any of these settings using command-line arguments.
# 
# Configuration Priority (highest to lowest):
# 1. Command-line arguments
# 2. Environment variables
# 3. This configuration file
# 4. Default values
# 
# ═══════════════════════════════════════════════════════════════════════════════

# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ MODEL CONFIGURATION                                                         │
# └─────────────────────────────────────────────────────────────────────────────┘
model:
  # Path to the GGUF model file (required)
  # Examples:
  #   - "./models/llama-2-7b-chat.gguf"
  #   - "./models/llama3.1n-4b.gguf"
  #   - "./models/qwen2.5-7b-instruct.gguf"
  path: "./models/llama3.1n-4b.gguf"
  
  # Context window size (number of tokens)
  # Recommended values:
  #   - 2048: Fast inference, basic conversations
  #   - 4096: Balanced performance and context
  #   - 8192: Large context, slower inference
  context_length: 4096
  
  # Number of CPU threads to use (null = auto-detect)
  # Set to number of CPU cores for optimal performance
  threads: null
  
  # Number of layers to offload to GPU (0 = CPU only)
  # Increase for better GPU utilization if available
  # Set to -1 to offload all layers to GPU
  gpu_layers: 0
  
  # ┌─── Sampling Parameters ───┐
  # Fine-tune response generation
  temperature: 0.7        # Creativity/randomness (0.0-2.0)
                         # 0.1-0.3: Focused, deterministic
                         # 0.7-0.9: Balanced creativity
                         # 1.0+: Very creative, unpredictable
  
  max_tokens: 2048        # Maximum tokens to generate
  top_p: 0.9             # Nucleus sampling (0.1-1.0)
  top_k: 40              # Top-k sampling limit
  repeat_penalty: 1.1    # Repetition penalty (1.0-1.3)

# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ DOCUMENT PROCESSING                                                         │
# └─────────────────────────────────────────────────────────────────────────────┘
documents:
  # Path to folder containing documents
  # The application will recursively scan this folder for supported files
  folder_path: "./documents"
  
  # ┌─── Text Chunking Parameters ───┐
  # Optimize for your document types and model context
  chunk_size: 1000       # Size of each text chunk (characters)
                         # Smaller: More precise retrieval
                         # Larger: More context per chunk
  
  chunk_overlap: 200     # Overlap between chunks (characters)
                         # Prevents information loss at boundaries
  
  # ┌─── Supported File Types ───┐
  # Automatically detected and processed:
  # • .txt  - Plain text files
  # • .pdf  - PDF documents
  # • .docx - Microsoft Word documents
  # • .doc  - Legacy Word documents
  # • .md   - Markdown files
  # 
  # Files are processed recursively from the folder

# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ EMBEDDING MODEL CONFIGURATION                                               │
# └─────────────────────────────────────────────────────────────────────────────┘
embeddings:
  # Sentence transformer model for generating embeddings
  # Choose based on your performance and quality requirements
  
  # ┌─── Recommended Models ───┐
  # Fast & Efficient (384 dimensions):
  #   - "all-MiniLM-L6-v2"              # Best balance of speed/quality
  #   - "multi-qa-MiniLM-L6-cos-v1"     # Optimized for Q&A tasks
  #   - "paraphrase-MiniLM-L6-v2"       # Good for paraphrase detection
  # 
  # High Quality (768 dimensions):
  #   - "all-mpnet-base-v2"             # Best overall quality
  #   - "sentence-transformers/all-distilroberta-v1"  # Alternative high-quality
  # 
  # Multilingual Support:
  #   - "paraphrase-multilingual-MiniLM-L12-v2"  # 50+ languages
  #   - "distiluse-base-multilingual-cased"      # 15+ languages
  # 
  # Local Models:
  #   - "./embeddings/my-custom-model"   # Local custom model
  #   - "Qwen3-0.6B"                    # From embeddings/ folder
  
  model: "all-MiniLM-L6-v2"
  
  # Embedding dimensions (auto-detected from model)
  # Uncomment to override auto-detection
  # dimensions: 384  # for all-MiniLM-L6-v2
  # dimensions: 768  # for all-mpnet-base-v2

# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ VECTOR STORE CONFIGURATION                                                  │
# └─────────────────────────────────────────────────────────────────────────────┘
vector_store:
  # ChromaDB collection name
  # Use descriptive names for different document sets
  collection_name: "documents"
  
  # Persistence settings
  # Vector database will be saved to this directory
  persist_directory: "./chroma_db"
  
  # Distance metric for similarity search
  # • cosine: Best for most text applications (recommended)
  # • euclidean: Good for numerical data
  # • manhattan: Alternative distance metric
  distance_metric: "cosine"

# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ RAG (RETRIEVAL-AUGMENTED GENERATION) CONFIGURATION                         │
# └─────────────────────────────────────────────────────────────────────────────┘
rag:
  # Number of documents to retrieve for context
  # More documents = more context but slower processing
  # Recommended: 3-7 for most use cases
  retrieve_count: 5
  
  # Minimum similarity threshold for retrieved documents
  # Higher values = more relevant but fewer results
  # Range: 0.0 (all results) to 1.0 (exact matches only)
  similarity_threshold: 0.7
  
  # Maximum context length (in characters)
  # Adjust based on your model's context window
  max_context_length: 4000
  
  # Context combination strategy
  # • concatenate: Simple joining (fastest)
  # • summarize: AI-powered summarization (slower)
  # • rank: Reorder by relevance (balanced)
  context_strategy: "concatenate"

# MCP (Model Context Protocol) Configuration
mcp:
  # Enable MCP server functionality
  enabled: false
  
  # MCP server settings
  server:
    host: "localhost"
    port: 8000
    
  # MCP tools configuration
  tools:
    document_search: true
    document_upload: true
    chat_history: true



# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ USER INTERFACE CONFIGURATION                                                │
# └─────────────────────────────────────────────────────────────────────────────┘
ui:
  # System prompt for the assistant
  # Customize the AI's behavior and personality
  system_prompt: "You are a helpful assistant that answers questions based on the provided context. If the context doesn't contain relevant information, say so clearly. Always be accurate, concise, and cite your sources when possible."
  
  # Chat template format
  # • auto: Automatically detect from model
  # • chatml: ChatML format (recommended for most models)
  # • llama2: Llama 2 chat format
  # • alpaca: Alpaca instruction format
  chat_template: "auto"
  
  # ┌─── Interactive Mode Settings ───┐
  interactive: true          # Enable interactive chat mode
  
  # ┌─── Display Settings ───┐
  verbose: false            # Show detailed processing information
  show_sources: true        # Display source documents for answers
  show_timing: false        # Show response generation timing
  streaming: true           # Enable real-time response streaming
  
  # ┌─── Color Scheme ───┐
  # Terminal output colors for better readability
  colors:
    user: "blue"           # User input color
    assistant: "green"     # AI response color
    system: "yellow"       # System messages color
    error: "red"           # Error messages color
    info: "cyan"           # Information messages color

# Logging Configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: "INFO"
  
  # Log file path (null = console only)
  file: null
  
  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # Rotate log files
  rotation:
    enabled: false
    max_size: "10MB"
    backup_count: 5

# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ PERFORMANCE CONFIGURATION                                                   │
# └─────────────────────────────────────────────────────────────────────────────┘
performance:
  # ┌─── Batch Processing ───┐
  batch_size: 512          # Embedding batch size (adjust for memory)
  
  # ┌─── Memory Management ───┐
  use_mlock: false         # Lock model in memory (prevents swapping)
                          # Enable for production, disable for development
  use_mmap: true          # Use memory mapping (recommended)
                          # Reduces memory usage for large models
  
  # ┌─── Response Caching ───┐
  cache:
    enabled: true         # Enable response caching for faster repeated queries
    max_size: 1000        # Maximum cached responses
    ttl: 3600            # Cache time-to-live (seconds)
                          # 3600 = 1 hour, 86400 = 24 hours
  
  # ┌─── Parallel Processing ───┐
  max_workers: 4          # Number of worker threads for document processing
                          # Set to number of CPU cores for optimal performance
  
  # ┌─── Optimization Settings ───┐
  preload_embeddings: true  # Preload embedding model at startup
  lazy_loading: false      # Load components on-demand vs at startup

# Development Configuration
development:
  # Debug settings
  debug: false
  profile: false
  
  # Hot reload for development
  hot_reload: false
  
  # API settings for development
  api:
    enabled: false
    host: "localhost"
    port: 8080
    cors_enabled: true

# Advanced Features
advanced:
  # Experimental features
  experimental:
    multi_modal: false
    streaming: true
    function_calling: false
  
  # Custom plugins
  plugins:
    enabled: false
    directory: "./plugins"
  
  # Integration settings
  integrations:
    langchain: false
    llamaindex: false
    openai_api: false

# Security Configuration
security:
  # Input validation
  max_input_length: 10000
  
  # Rate limiting
  rate_limit:
    enabled: false
    requests_per_minute: 60
  
  # Content filtering
  content_filter:
    enabled: false
    blocked_patterns: []
  
  # API security
  api_key: null
  allowed_origins: ["*"]

# Environment-specific overrides
# These can be used to override settings based on environment
environments:
  development:
    logging:
      level: "DEBUG"
    ui:
      verbose: true
  
  production:
    logging:
      level: "WARNING"
      file: "./logs/docuchat.log"
    performance:
      cache:
        enabled: true
    security:
      rate_limit:
        enabled: true
  
  testing:
    model:
      max_tokens: 100
    rag:
      retrieve_count: 2
